{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This code uses an ANN to predict outcomes of AFL matches in 2018 based on player data from 2012-2017.'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afl = pd.read_csv('C:/path/to/your/csv/file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "afl = pd.read_csv('C:/Users/the_n/OneDrive/Documents/Coding/stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63712 entries, 0 to 63711\n",
      "Data columns (total 37 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Team                    63712 non-null  object \n",
      " 1   Player                  63712 non-null  object \n",
      " 2   D.O.B                   63712 non-null  object \n",
      " 3   Height                  63712 non-null  int64  \n",
      " 4   Weight                  63712 non-null  int64  \n",
      " 5   Position                63712 non-null  object \n",
      " 6   Season                  63712 non-null  int64  \n",
      " 7   Round                   63712 non-null  object \n",
      " 8   Date                    63624 non-null  object \n",
      " 9   Score                   63624 non-null  float64\n",
      " 10  Margin                  63624 non-null  float64\n",
      " 11  WinLoss                 63624 non-null  object \n",
      " 12  Opposition              63624 non-null  object \n",
      " 13  Venue                   63624 non-null  object \n",
      " 14  Disposals               63712 non-null  int64  \n",
      " 15  Kicks                   63712 non-null  int64  \n",
      " 16  Marks                   63712 non-null  int64  \n",
      " 17  Handballs               63712 non-null  int64  \n",
      " 18  Goals                   63712 non-null  int64  \n",
      " 19  Behinds                 63712 non-null  int64  \n",
      " 20  Hitouts                 63712 non-null  int64  \n",
      " 21  Tackles                 63712 non-null  int64  \n",
      " 22  Rebound50s              63712 non-null  int64  \n",
      " 23  Inside50s               63712 non-null  int64  \n",
      " 24  Clearances              63712 non-null  int64  \n",
      " 25  Clangers                63712 non-null  int64  \n",
      " 26  FreesFor                63712 non-null  int64  \n",
      " 27  FreesAgainst            63712 non-null  int64  \n",
      " 28  BrownlowVotes           63712 non-null  int64  \n",
      " 29  ContendedPossessions    63712 non-null  int64  \n",
      " 30  UncontendedPossessions  63712 non-null  int64  \n",
      " 31  ContestedMarks          63712 non-null  int64  \n",
      " 32  MarksInside50           63712 non-null  int64  \n",
      " 33  OnePercenters           63712 non-null  int64  \n",
      " 34  Bounces                 63712 non-null  int64  \n",
      " 35  GoalAssists             63712 non-null  int64  \n",
      " 36  PercentPlayed           63712 non-null  int64  \n",
      "dtypes: float64(2), int64(26), object(9)\n",
      "memory usage: 18.0+ MB\n"
     ]
    }
   ],
   "source": [
    "afl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "afl = afl.dropna(axis=0)\n",
    "afl = afl[afl['WinLoss'] != 'D']\n",
    "afl['D.O.B'] = pd.to_datetime(afl['D.O.B'])\n",
    "afl['Date'] = pd.to_datetime(afl['Date'])\n",
    "age_in_days = (afl['Date']-afl['D.O.B'])\n",
    "age_in_years = age_in_days.dt.days/365.2425\n",
    "afl['Age'] = age_in_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "grouped_data = afl.groupby(['Team','Season','Round','WinLoss','Opposition','Venue'])['Player'].apply(list).reset_index()\n",
    "#print(grouped_data[\"Player\"])\n",
    "#grouped_data['Player'] = grouped_data['Player'].apply(np.random.shuffle)\n",
    "#grouped_data = grouped_data.to_numpy()\n",
    "players = grouped_data['Player'].to_numpy()\n",
    "\n",
    "for item in players:\n",
    "    random.shuffle(item)\n",
    "    \n",
    "grouped_data['Player'] = players\n",
    "grouped_data = grouped_data.to_numpy()\n",
    "#print(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adelaide', 2012, 'PF', 'L', 'Hawthorn', 'M.C.G.',\n",
       "       list(['Henderson, Ricky', 'Petrenko, Jared', 'Doughty, Michael', 'Reilly, Brent', 'Wright, Matthew', 'Vince, Bernie', 'Dangerfield, Patrick', 'Thompson, Luke', 'Thompson, Scott', 'Smith, Brodie', 'Jacobs, Sam', 'van Berlo, Nathan', 'Walker, Taylor', 'Sloane, Rory', 'Douglas, Richard', 'Callinan, Ian', 'Mackay, David', 'Tippett, Kurt', 'Porplyzia, Jason', 'Otten, Andy', 'Johncock, Graham', 'Rutten, Ben'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct1 = ColumnTransformer([('encoder',OneHotEncoder(),[3,4,5])], remainder='passthrough',sparse_threshold=0)\n",
    "grouped_data = ct1.fit_transform(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,\n",
       "       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "       0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "       0.0, 0.0, 0.0, 'Adelaide', 2012, 'PF',\n",
       "       list(['Henderson, Ricky', 'Petrenko, Jared', 'Doughty, Michael', 'Reilly, Brent', 'Wright, Matthew', 'Vince, Bernie', 'Dangerfield, Patrick', 'Thompson, Luke', 'Thompson, Scott', 'Smith, Brodie', 'Jacobs, Sam', 'van Berlo, Nathan', 'Walker, Taylor', 'Sloane, Rory', 'Douglas, Richard', 'Callinan, Ian', 'Mackay, David', 'Tippett, Kurt', 'Porplyzia, Jason', 'Otten, Andy', 'Johncock, Graham', 'Rutten, Ben'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [x for x in grouped_data if x[43]<2018]\n",
    "test = [x for x in grouped_data if x[43]==2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([x[0] for x in training])\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = np.array([x[0] for x in test])\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_stats is each players' average stats between 2012 and 2017.\n",
    "player_stats = afl[afl['Season'] < 2018].groupby('Player',sort=False).mean()\n",
    "player_stats = player_stats.reset_index()\n",
    "player_stats = player_stats.drop(['Season','Score','Margin'],axis=1)\n",
    "player_stats = player_stats.to_numpy()\n",
    "\n",
    "# Player average stats for 2018\n",
    "player_stats_2018 = afl[afl['Season'] == 2018].groupby('Player',sort=False).mean()\n",
    "player_stats_2018 = player_stats_2018.reset_index()\n",
    "player_stats_2018 = player_stats_2018.drop(['Season','Score','Margin'],axis=1)\n",
    "player_stats_2018 = player_stats_2018.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Atkins, Rory', 186.0, 85.0, 19.910714285714285,\n",
       "       11.053571428571429, 4.357142857142857, 8.857142857142858,\n",
       "       0.4642857142857143, 0.35714285714285715, 0.10714285714285714,\n",
       "       1.8928571428571428, 1.8392857142857142, 3.482142857142857,\n",
       "       1.8392857142857142, 2.607142857142857, 0.4107142857142857, 0.75,\n",
       "       0.19642857142857142, 6.142857142857143, 13.982142857142858,\n",
       "       0.30357142857142855, 0.3392857142857143, 1.2321428571428572,\n",
       "       0.7678571428571429, 0.625, 78.32142857142857, 22.247987882805838],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_stats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(player_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include opposition and venue in features\n",
    "opp_teams_train = np.array([x[2:42] for x in training])\n",
    "opp_teams_test = np.array([x[2:42] for x in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,\n",
       "       0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "       0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "       0.0], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opp_teams_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [0]*len(training)\n",
    "for i in range(0,len(training)):\n",
    "    player_list = []\n",
    "    j = 0\n",
    "    for j in range(0,len(player_stats)):\n",
    "        if player_stats[j][0] in training[i][-1]:\n",
    "            player_list.append(player_stats[j][1:])\n",
    "    X_train[i] = player_list\n",
    "    \n",
    "X_train = [np.concatenate(x) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([181.0, 79.0, 20.346774193548388, 12.725806451612904,\n",
       "       3.4919354838709675, 7.620967741935484, 0.6693548387096774,\n",
       "       0.5645161290322581, 0.06451612903225806, 3.870967741935484,\n",
       "       1.314516129032258, 4.306451612903226, 3.661290322580645,\n",
       "       2.4274193548387095, 1.0, 0.9435483870967742, 0.03225806451612903,\n",
       "       8.435483870967742, 11.85483870967742, 0.22580645161290322,\n",
       "       0.5161290322580645, 1.3951612903225807, 0.532258064516129,\n",
       "       0.7661290322580645, 81.44354838709677, 27.993200275468777, 200.0,\n",
       "       102.0, 14.18978102189781, 8.021897810218977, 3.7664233576642334,\n",
       "       6.1678832116788325, 0.2846715328467153, 0.24817518248175183,\n",
       "       33.91970802919708, 2.065693430656934, 1.0656934306569343,\n",
       "       1.9562043795620438, 2.335766423357664, 1.8832116788321167,\n",
       "       0.8905109489051095, 0.7956204379562044, 0.19708029197080293,\n",
       "       6.189781021897811, 7.956204379562044, 0.9124087591240876,\n",
       "       0.27007299270072993, 2.627737226277372, 0.021897810218978103,\n",
       "       0.3722627737226277, 85.94890510948905, 26.694953043146167, 181.0,\n",
       "       77.0, 16.917355371900825, 9.239669421487603, 3.024793388429752,\n",
       "       7.677685950413223, 0.32231404958677684, 0.34710743801652894,\n",
       "       0.049586776859504134, 3.8595041322314048, 1.6942148760330578,\n",
       "       2.8264462809917354, 1.3140495867768596, 1.9834710743801653,\n",
       "       0.7107438016528925, 0.7024793388429752, 0.008264462809917356,\n",
       "       5.661157024793388, 11.40495867768595, 0.14049586776859505,\n",
       "       0.17355371900826447, 1.2231404958677685, 0.9504132231404959,\n",
       "       0.38016528925619836, 78.0495867768595, 26.36142850184954, 192.0,\n",
       "       94.0, 13.339285714285714, 7.214285714285714, 4.321428571428571,\n",
       "       6.125, 0.5892857142857143, 0.21428571428571427, 0.7678571428571429,\n",
       "       2.3035714285714284, 1.3928571428571428, 1.4464285714285714,\n",
       "       0.42857142857142855, 1.5714285714285714, 0.6964285714285714,\n",
       "       0.6428571428571429, 0.017857142857142856, 4.928571428571429,\n",
       "       8.607142857142858, 0.8392857142857143, 0.625, 4.142857142857143,\n",
       "       0.08928571428571429, 0.35714285714285715, 85.21428571428571,\n",
       "       25.434862747743917, 182.0, 83.0, 23.53435114503817,\n",
       "       12.206106870229007, 4.358778625954199, 11.32824427480916,\n",
       "       0.6717557251908397, 0.35877862595419846, 0.648854961832061,\n",
       "       5.938931297709924, 1.4732824427480915, 4.114503816793893,\n",
       "       4.534351145038168, 2.595419847328244, 1.4732824427480915,\n",
       "       1.0076335877862594, 0.6412213740458015, 12.099236641221374,\n",
       "       11.770992366412214, 0.5877862595419847, 0.4198473282442748,\n",
       "       2.5267175572519083, 0.3511450381679389, 0.7022900763358778,\n",
       "       82.87786259541984, 24.745057228787715, 189.0, 90.0, 19.6171875,\n",
       "       12.3828125, 4.21875, 7.234375, 0.34375, 0.3359375, 0.03125,\n",
       "       1.859375, 3.7890625, 3.34375, 0.8828125, 2.2578125, 0.4921875,\n",
       "       0.5078125, 0.1015625, 5.3828125, 13.0390625, 0.2890625, 0.125,\n",
       "       2.21875, 1.1640625, 0.3671875, 82.5078125, 22.98733889128456,\n",
       "       193.0, 102.0, 14.392523364485982, 10.177570093457945,\n",
       "       6.570093457943925, 4.214953271028038, 2.485981308411215,\n",
       "       1.6822429906542056, 0.028037383177570093, 1.560747663551402,\n",
       "       0.09345794392523364, 3.5046728971962615, 0.09345794392523364, 3.0,\n",
       "       0.9065420560747663, 1.3177570093457944, 0.32710280373831774,\n",
       "       6.420560747663552, 8.242990654205608, 1.5700934579439252,\n",
       "       2.7196261682242993, 1.3364485981308412, 0.08411214953271028,\n",
       "       1.0934579439252337, 89.6355140186916, 25.017740421979273, 185.0,\n",
       "       86.0, 26.336363636363636, 13.354545454545455, 3.7636363636363637,\n",
       "       12.981818181818182, 0.3, 0.34545454545454546, 0.14545454545454545,\n",
       "       5.4818181818181815, 1.4090909090909092, 3.9, 6.163636363636364,\n",
       "       3.918181818181818, 1.3, 1.8545454545454545, 0.5909090909090909,\n",
       "       12.481818181818182, 13.872727272727273, 0.2545454545454545,\n",
       "       0.24545454545454545, 1.1181818181818182, 0.02727272727272727,\n",
       "       0.4727272727272727, 83.96363636363637, 31.2333213238776, 178.0,\n",
       "       75.0, 17.92436974789916, 10.428571428571429, 3.957983193277311,\n",
       "       7.495798319327731, 0.8151260504201681, 0.3949579831932773, 0.0,\n",
       "       3.2436974789915967, 1.1176470588235294, 2.7394957983193278,\n",
       "       2.2184873949579833, 1.8991596638655461, 1.1680672268907564,\n",
       "       0.5378151260504201, 0.03361344537815126, 7.067226890756302,\n",
       "       10.630252100840336, 0.19327731092436976, 0.6554621848739496,\n",
       "       0.9327731092436975, 0.3697478991596639, 0.680672268907563,\n",
       "       81.15126050420169, 25.044279606337287, 189.0, 92.0,\n",
       "       27.248175182481752, 14.489051094890511, 4.197080291970803,\n",
       "       12.75912408759124, 1.167883211678832, 0.8686131386861314,\n",
       "       0.9124087591240876, 4.313868613138686, 1.3868613138686132,\n",
       "       5.642335766423358, 6.510948905109489, 3.510948905109489,\n",
       "       1.8686131386861313, 1.1240875912408759, 1.1167883211678833,\n",
       "       15.773722627737227, 11.89051094890511, 1.072992700729927,\n",
       "       0.8321167883211679, 1.3503649635036497, 0.7664233576642335,\n",
       "       0.635036496350365, 84.74452554744525, 24.718184184100465, 188.0,\n",
       "       91.0, 18.974025974025974, 11.688311688311689, 5.662337662337662,\n",
       "       7.285714285714286, 0.37662337662337664, 0.2857142857142857,\n",
       "       0.012987012987012988, 1.5194805194805194, 2.6623376623376624,\n",
       "       2.5064935064935066, 0.8831168831168831, 1.9610389610389611,\n",
       "       0.4155844155844156, 0.4025974025974026, 0.1038961038961039,\n",
       "       4.779220779220779, 13.857142857142858, 0.33766233766233766,\n",
       "       0.3116883116883117, 2.1298701298701297, 0.7272727272727273,\n",
       "       0.4025974025974026, 77.50649350649351, 26.400499436906752, 186.0,\n",
       "       86.0, 22.846774193548388, 15.040322580645162, 3.879032258064516,\n",
       "       7.806451612903226, 0.5080645161290323, 0.4435483870967742,\n",
       "       0.25806451612903225, 3.217741935483871, 2.5403225806451615,\n",
       "       3.3225806451612905, 3.6370967741935485, 2.903225806451613,\n",
       "       0.9596774193548387, 0.9919354838709677, 0.25806451612903225,\n",
       "       8.209677419354838, 14.274193548387096, 0.20161290322580644,\n",
       "       0.2661290322580645, 1.0161290322580645, 0.33064516129032256,\n",
       "       0.29838709677419356, 82.95967741935483, 29.18542629764097, 202.0,\n",
       "       106.0, 11.726315789473684, 7.063157894736842, 4.073684210526316,\n",
       "       4.663157894736842, 1.8526315789473684, 1.0842105263157895,\n",
       "       12.294736842105262, 2.2421052631578946, 0.23157894736842105,\n",
       "       1.7894736842105263, 1.3368421052631578, 2.1473684210526316,\n",
       "       1.2526315789473683, 1.263157894736842, 0.08421052631578947,\n",
       "       7.3052631578947365, 4.621052631578947, 1.8210526315789475,\n",
       "       2.0526315789473686, 2.0526315789473686, 0.010526315789473684,\n",
       "       0.43157894736842106, 79.46315789473684, 27.451293174722057, 184.0,\n",
       "       83.0, 16.71641791044776, 10.626865671641792, 3.91044776119403,\n",
       "       6.08955223880597, 0.373134328358209, 0.40298507462686567,\n",
       "       0.05970149253731343, 4.029850746268656, 1.5820895522388059,\n",
       "       2.91044776119403, 2.08955223880597, 2.5671641791044775,\n",
       "       0.7164179104477612, 1.0, 0.0, 6.268656716417911,\n",
       "       10.044776119402986, 0.3582089552238806, 0.2835820895522388,\n",
       "       1.2238805970149254, 0.029850746268656716, 0.4925373134328358,\n",
       "       83.7910447761194, 27.403915554366407, 177.0, 85.0,\n",
       "       15.523809523809524, 9.619047619047619, 3.9285714285714284,\n",
       "       5.904761904761905, 1.0476190476190477, 0.6428571428571429,\n",
       "       0.07142857142857142, 2.3095238095238093, 0.7380952380952381,\n",
       "       2.4285714285714284, 1.6428571428571428, 2.238095238095238,\n",
       "       0.30952380952380953, 0.8333333333333334, 0.047619047619047616,\n",
       "       6.809523809523809, 8.523809523809524, 0.7619047619047619,\n",
       "       1.119047619047619, 1.119047619047619, 0.14285714285714285,\n",
       "       0.5952380952380952, 74.9047619047619, 28.109113416819945, 190.0,\n",
       "       98.0, 11.8, 6.0, 4.85, 5.8, 0.05, 0.03333333333333333,\n",
       "       0.08333333333333333, 1.4666666666666666, 1.65, 0.55, 0.2,\n",
       "       1.0833333333333333, 0.48333333333333334, 0.5833333333333334, 0.0,\n",
       "       3.7666666666666666, 8.083333333333334, 0.5333333333333333,\n",
       "       0.016666666666666666, 3.7, 0.15, 0.05, 87.83333333333333,\n",
       "       29.871980031531564, 183.0, 85.0, 18.74468085106383,\n",
       "       12.851063829787234, 5.659574468085107, 5.8936170212765955,\n",
       "       0.0851063829787234, 0.06382978723404255, 0.0, 2.425531914893617,\n",
       "       3.2127659574468086, 1.4680851063829787, 0.851063829787234,\n",
       "       2.3191489361702127, 0.574468085106383, 1.0851063829787233, 0.0,\n",
       "       5.787234042553192, 12.148936170212766, 0.8085106382978723,\n",
       "       0.06382978723404255, 2.404255319148936, 1.0212765957446808,\n",
       "       0.14893617021276595, 83.82978723404256, 29.248769288955355, 178.0,\n",
       "       81.0, 10.725, 6.375, 2.325, 4.35, 0.825, 0.7, 0.05, 4.175, 0.275,\n",
       "       1.925, 0.9, 1.625, 0.775, 0.7, 0.0, 5.2, 5.175, 0.25, 0.625, 1.125,\n",
       "       0.275, 0.675, 75.825, 23.02066435313525, 194.0, 88.0,\n",
       "       9.88888888888889, 4.888888888888889, 2.888888888888889, 5.0, 0.0,\n",
       "       0.0, 0.0, 2.3333333333333335, 1.7777777777777777,\n",
       "       0.4444444444444444, 0.3333333333333333, 1.1111111111111112,\n",
       "       0.7777777777777778, 0.4444444444444444, 0.0, 3.2222222222222223,\n",
       "       6.777777777777778, 0.3333333333333333, 0.0, 5.222222222222222,\n",
       "       0.6666666666666666, 0.0, 89.11111111111111, 22.842053947415454,\n",
       "       178.0, 86.0, 11.428571428571429, 8.047619047619047,\n",
       "       3.5714285714285716, 3.380952380952381, 1.0, 1.2380952380952381,\n",
       "       0.0, 2.4285714285714284, 1.1904761904761905, 1.9047619047619047,\n",
       "       0.47619047619047616, 1.7142857142857142, 0.9523809523809523,\n",
       "       0.7142857142857143, 0.047619047619047616, 4.095238095238095,\n",
       "       7.285714285714286, 0.23809523809523808, 0.8571428571428571,\n",
       "       1.0952380952380953, 0.09523809523809523, 0.23809523809523808,\n",
       "       75.9047619047619, 29.765221214737636, 172.0, 74.0,\n",
       "       12.448275862068966, 7.551724137931035, 2.8620689655172415,\n",
       "       4.896551724137931, 1.6206896551724137, 0.7931034482758621, 0.0,\n",
       "       1.7586206896551724, 0.3103448275862069, 2.2413793103448274,\n",
       "       0.7241379310344828, 1.6206896551724137, 0.5517241379310345,\n",
       "       0.6551724137931034, 0.034482758620689655, 5.517241379310345,\n",
       "       6.482758620689655, 0.3793103448275862, 1.103448275862069,\n",
       "       0.6551724137931034, 0.2413793103448276, 0.7931034482758621,\n",
       "       80.03448275862068, 29.739051499322724, 177.0, 81.0,\n",
       "       15.541666666666666, 8.583333333333334, 4.166666666666667,\n",
       "       6.958333333333333, 0.08333333333333333, 0.041666666666666664, 0.0,\n",
       "       3.0833333333333335, 1.9583333333333333, 1.2083333333333333,\n",
       "       0.8333333333333334, 1.4583333333333333, 0.875, 0.625, 0.0, 5.5,\n",
       "       9.75, 0.2916666666666667, 0.0, 3.8333333333333335,\n",
       "       0.041666666666666664, 0.20833333333333334, 85.33333333333333,\n",
       "       32.895040053297926], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X_train[0]))\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''modified: player_stats_2018 -> player_stats to test performance'''\n",
    "X_test = [0]*len(test)\n",
    "for i in range(0,len(test)):\n",
    "    player_list = []\n",
    "    j = 0\n",
    "    for j in range(0,len(player_stats_2018)):\n",
    "        if player_stats_2018[j][0] in test[i][-1]:\n",
    "            player_list.append(player_stats_2018[j][1:])\n",
    "    X_test[i] = player_list\n",
    "    \n",
    "X_test = [np.concatenate(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "scaled_X_train = sc.fit_transform(X_train)\n",
    "scaled_X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = np.hstack((scaled_X_train, opp_teams_train))\n",
    "scaled_X_test = np.hstack((scaled_X_test, opp_teams_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = np.asarray(scaled_X_train).astype(np.float32)\n",
    "scaled_X_test = np.asarray(scaled_X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.44964647e-01 -1.06107175e+00  6.18809342e-01  1.21086979e+00\n",
      " -6.62710309e-01  2.24780035e-03 -1.99093353e-02  2.76421607e-01\n",
      " -2.37170607e-01  5.80942214e-01 -1.59369335e-01  1.61617553e+00\n",
      "  1.15693474e+00  4.88989621e-01  5.73852718e-01  5.92228711e-01\n",
      " -5.41615188e-01  8.38677347e-01  3.50265175e-01 -5.74591875e-01\n",
      " -1.45324454e-01 -3.24574262e-01  1.82839751e-01  1.12314725e+00\n",
      " -6.65854588e-02  1.48138237e+00  1.48642302e+00  1.49926674e+00\n",
      " -4.98186707e-01 -4.20792162e-01 -2.46276647e-01 -4.70008254e-01\n",
      " -6.96794450e-01 -6.69697583e-01  4.98701954e+00 -8.92397702e-01\n",
      " -3.31930578e-01 -4.39176649e-01  2.88001388e-01 -4.91637856e-01\n",
      "  8.16050619e-02 -1.67277917e-01  1.65571541e-01 -1.78655699e-01\n",
      " -6.01860940e-01  8.02277267e-01 -6.12190485e-01  3.75586897e-01\n",
      " -9.59012747e-01 -3.23338598e-01  6.65037215e-01  6.78989708e-01\n",
      " -1.03061175e+00 -1.53985322e+00 -1.08733557e-01 -2.03033388e-01\n",
      " -1.09663534e+00  5.59475785e-03 -6.64329171e-01 -4.20438260e-01\n",
      " -2.77306497e-01  5.41319668e-01  3.00944179e-01  1.83034271e-01\n",
      " -3.76473665e-01 -5.24111450e-01 -4.81638700e-01 -4.90642488e-01\n",
      " -7.02217877e-01 -4.64239508e-01  2.39514917e-01 -8.06424201e-01\n",
      " -7.47153819e-01 -4.89375085e-01  1.35540581e+00 -3.55426192e-01\n",
      " -8.40888798e-01  7.14284003e-01  6.63838267e-01  7.10492611e-01\n",
      " -9.15145695e-01 -1.06945467e+00  9.49373916e-02 -5.97343147e-01\n",
      " -1.59415156e-01 -7.55618632e-01 -1.35770440e-01 -7.18832970e-01\n",
      " -1.77933455e-01 -1.12887073e+00 -9.39356863e-01 -1.30753911e+00\n",
      " -5.06963849e-01 -6.70822501e-01 -7.56659210e-01 -8.35953534e-01\n",
      " -6.99106634e-01  6.53213382e-01  1.33452029e-03  1.52091658e+00\n",
      " -9.47467566e-01 -3.99624944e-01  3.54181796e-01  2.55315810e-01\n",
      " -8.51938665e-01 -7.07131505e-01  1.03791142e+00  7.05284417e-01\n",
      " -7.20451819e-04  1.07187605e+00  1.05098166e-01 -2.27530226e-01\n",
      " -2.03575104e-01  2.39203715e+00 -2.49277234e-01  1.58753002e+00\n",
      "  1.57932985e+00  7.31096625e-01  1.86989617e+00  7.76473284e-01\n",
      "  2.35184526e+00  2.18377566e+00  1.51864633e-01  1.00950614e-01\n",
      " -1.98011607e-01  2.35682294e-01 -9.86503884e-02  1.24439156e+00\n",
      " -9.60637704e-02  3.11785620e-02  3.44555490e-02  1.12101868e-01\n",
      "  3.54700118e-01  8.37802410e-01 -1.48756444e-01 -1.73914433e-01\n",
      " -4.94824469e-01 -3.25564891e-01 -2.97642708e-01 -9.70727980e-01\n",
      "  2.31481767e+00  8.27246070e-01 -5.93910754e-01  4.50896621e-02\n",
      " -1.23847091e+00 -1.17095256e+00 -3.91447008e-01 -6.27734900e-01\n",
      "  6.32392526e-01 -5.64945579e-01 -6.71717346e-01 -6.55303448e-02\n",
      "  2.43165851e+00 -2.62087286e-01 -2.59956449e-01 -6.74652100e-01\n",
      "  6.43078983e-01  1.64007103e+00 -6.80006504e-01  5.99085949e-02\n",
      "  1.85188627e+00 -1.17036116e+00  2.88822389e+00  3.42196655e+00\n",
      " -2.72375733e-01 -1.19871175e+00 -1.64348304e+00  1.00260067e+00\n",
      " -9.99738455e-01  1.57358468e+00  1.44186825e-01  1.87392402e+00\n",
      "  7.59423137e-01 -1.39216766e-01 -8.02510619e-01  2.41130853e+00\n",
      "  3.08959126e+00 -5.20521045e-01 -7.49287367e-01  2.83290648e+00\n",
      "  1.10346174e+00  8.93063471e-02 -3.60429883e-01 -2.19957486e-01\n",
      "  1.71149230e+00  1.20727861e+00 -3.86529356e-01  1.68486488e+00\n",
      " -5.29982090e-01 -2.53453553e-01 -2.68454850e-01  1.82692695e+00\n",
      " -2.44144395e-01  1.35119069e+00  2.38991880e+00  3.28572750e+00\n",
      "  1.23965359e+00  3.48989034e+00  2.19455791e+00  2.19257092e+00\n",
      "  8.64397168e-01 -5.27735114e-01 -4.70198125e-01 -6.11749589e-01\n",
      " -8.11892033e-01  2.36427635e-01  2.03139991e-01  2.16039371e+00\n",
      " -1.26009095e+00 -1.47076905e+00  2.07484141e-02  1.72180429e-01\n",
      " -2.21961781e-01 -1.29297227e-01  4.67381090e-01 -1.68542136e-02\n",
      " -3.01787615e-01  9.79537591e-02 -5.59098065e-01  2.79815882e-01\n",
      "  1.88389331e-01 -6.65618360e-01  7.55025625e-01 -8.96841109e-01\n",
      " -6.26820147e-01  1.20444901e-01 -1.01462148e-01 -6.72019958e-01\n",
      "  2.83885539e-01 -7.53802180e-01 -1.19006850e-01  1.24774337e+00\n",
      " -2.52997905e-01  1.09781019e-01  2.86590606e-01  5.68239748e-01\n",
      "  1.96399653e+00  1.51530802e+00 -1.55969232e-03  1.77988350e+00\n",
      "  1.11337280e+00  1.47838926e+00 -9.73054469e-02  9.45152462e-01\n",
      " -3.12535644e-01  3.05320358e+00  2.84112144e+00  2.14351583e+00\n",
      "  2.72312903e+00  8.57011676e-01  4.54315615e+00  3.74905396e+00\n",
      "  2.78833747e-01  1.29950941e+00  6.28301919e-01 -4.84864861e-01\n",
      "  5.06910026e-01  1.03947079e+00  3.81613821e-01  2.98698507e-02\n",
      "  1.01340607e-01  4.05258596e-01  3.15029025e-01  6.38361812e-01\n",
      "  1.26557577e+00 -9.41230133e-02 -3.84911418e-01 -3.88186306e-01\n",
      " -2.58880347e-01 -1.27290034e+00  9.10921216e-01  8.72308910e-02\n",
      " -5.37169218e-01 -4.88399804e-01 -1.18053830e+00 -1.35845160e+00\n",
      " -2.66944498e-01 -7.61122465e-01  9.45534289e-01 -3.42839777e-01\n",
      " -3.28653872e-01 -5.28588034e-02  5.16758680e-01 -4.43884246e-02\n",
      " -8.65342379e-01  5.01841247e-01 -2.45165199e-01 -2.78706223e-01\n",
      "  1.26850116e+00  2.03497577e+00 -2.16073632e-01  1.58334047e-01\n",
      " -1.73155040e-01  4.35282402e-02 -2.02439979e-01  5.99664897e-02\n",
      "  8.56520116e-01  1.00162184e+00  1.08744943e+00  1.17143393e+00\n",
      "  2.41715342e-01  4.45099831e-01  6.55607224e-01  6.72199845e-01\n",
      "  1.30446565e+00 -6.78817987e-01 -4.31898713e-01 -7.22849786e-01\n",
      " -1.82296962e-01 -4.71794963e-01  5.75136542e-02  1.33124435e+00\n",
      "  2.13223863e+00  2.28760815e+00 -1.09253228e+00 -9.34656680e-01\n",
      "  9.10455268e-03 -9.32466984e-01  2.12812710e+00  1.82384002e+00\n",
      "  1.87811649e+00 -7.01019287e-01 -1.28776813e+00 -6.23073697e-01\n",
      " -2.66670108e-01 -1.33478135e-01  1.04988933e+00  1.28187716e+00\n",
      " -2.78062016e-01  3.54107767e-01 -1.79108167e+00  2.82507730e+00\n",
      "  2.29449201e+00 -3.00438739e-02 -8.48353028e-01  1.22690238e-01\n",
      " -4.08859640e-01  8.17928791e-01 -5.09211063e-01 -6.32451832e-01\n",
      "  5.84636927e-02  5.00973165e-01 -6.76964223e-02 -3.80370498e-01\n",
      " -3.86969060e-01 -2.83601284e-02 -2.82783210e-01  7.78618097e-01\n",
      "  7.54626235e-03  6.12640321e-01  2.33642355e-01  7.03894377e-01\n",
      " -3.12508523e-01  4.99834955e-01 -6.92859173e-01 -1.21385315e-02\n",
      " -7.82594923e-03 -2.96932131e-01 -3.84998113e-01 -5.79293132e-01\n",
      " -7.94609487e-01  5.11222303e-01  3.49591911e-01  7.96355426e-01\n",
      " -1.53119612e+00 -3.97857845e-01 -1.60543516e-01  1.52830601e-01\n",
      " -6.41966239e-02 -4.42849517e-01  9.76986885e-01  7.60697603e-01\n",
      " -2.91233301e-01 -5.99036932e-01 -8.24669003e-01  1.38620377e-01\n",
      "  4.32913471e-03  1.75500378e-01 -1.43529820e+00  3.88624221e-02\n",
      " -3.63971174e-01  2.94949979e-01 -4.73004699e-01  6.88009143e-01\n",
      "  1.17498386e+00 -6.31863892e-01 -6.28557086e-01  1.01856613e+00\n",
      " -9.58716094e-01  9.17191684e-01  2.67472923e-01  1.17728186e+00\n",
      " -9.22951996e-01 -1.15699208e+00  7.85757959e-01 -4.61614698e-01\n",
      " -9.82222319e-01 -1.12078905e+00 -3.13614756e-01 -1.33208513e+00\n",
      "  1.20142996e-01 -1.75966787e+00 -9.35109794e-01 -1.94708025e+00\n",
      " -9.11858261e-01 -7.84181118e-01 -6.30994618e-01 -1.07433367e+00\n",
      " -5.71896970e-01  8.37616548e-02 -8.77766132e-01  1.04624820e+00\n",
      " -5.57133615e-01 -1.52711546e+00  9.47948515e-01  1.25768387e+00\n",
      " -7.01488495e-01 -4.13380116e-01  6.63876891e-01  1.45668566e+00\n",
      "  1.61917055e+00 -3.44850332e-01 -8.66941869e-01 -1.00954258e+00\n",
      " -3.19771409e-01 -3.66287142e-01  1.68424702e+00 -6.82577670e-01\n",
      " -4.42206383e-01  4.91066098e-01 -5.97313344e-01  8.90743792e-01\n",
      " -5.92196643e-01 -4.68898304e-02  7.66392231e-01  8.36656094e-01\n",
      " -7.58020818e-01  8.69255960e-02  1.57544267e+00 -9.34229076e-01\n",
      "  3.52214277e-01  1.06166518e+00 -1.35509503e+00 -9.34840381e-01\n",
      " -1.06943905e+00 -9.25110459e-01 -1.46190536e+00 -9.34480667e-01\n",
      "  6.34586930e-01  1.10466993e+00 -3.05785090e-01  1.24184716e+00\n",
      " -1.24654675e+00 -2.09857151e-01 -4.09569561e-01 -8.66403699e-01\n",
      " -3.68366502e-02 -3.47393632e-01 -6.12443149e-01 -3.14555168e-01\n",
      " -1.41187418e+00 -5.35260141e-01  2.82600611e-01 -6.69407010e-01\n",
      " -1.96893081e-01  1.43283808e+00 -6.19865835e-01 -6.12019181e-01\n",
      "  8.97117436e-01  1.84223000e-02 -1.17982244e+00 -1.38390028e+00\n",
      " -8.65662694e-01 -6.52515292e-01 -1.04944706e+00 -1.28700316e+00\n",
      " -3.16383958e-01 -4.58456516e-01  3.16194475e-01 -1.77054107e+00\n",
      " -8.15119863e-01 -1.79344869e+00 -1.31525332e-02 -1.18237412e+00\n",
      " -5.78060746e-01 -1.27040887e+00 -8.11834335e-01 -2.81072706e-01\n",
      " -9.17711258e-01  1.95997989e+00  7.15140104e-01 -1.60352027e+00\n",
      "  1.14714456e+00 -6.62878931e-01 -1.34476471e+00 -2.93025583e-01\n",
      " -7.99647987e-01 -1.56083405e-01 -2.19125345e-01 -1.26597691e+00\n",
      "  1.04376924e+00  2.94918466e+00 -3.38873804e-01 -2.99597085e-01\n",
      " -2.37962097e-01 -1.76177606e-01 -7.05588639e-01 -6.17885590e-01\n",
      "  5.51392317e-01 -3.37058365e-01 -1.76504582e-01 -8.14433217e-01\n",
      " -6.07588887e-01 -5.46337366e-01  8.42047930e-01 -6.88188255e-01\n",
      " -5.76614976e-01 -5.12049258e-01 -4.63523060e-01  9.65665281e-01\n",
      " -2.19331837e+00 -1.75345671e+00 -5.05438507e-01 -2.36384392e-01\n",
      " -7.33447194e-01 -6.46196365e-01  2.60041451e+00  1.61951101e+00\n",
      " -3.57517213e-01 -9.00337160e-01 -1.10027158e+00  3.51614416e-01\n",
      " -4.97649789e-01 -7.66390979e-01 -5.74428439e-01 -5.50410688e-01\n",
      " -2.00855836e-01 -4.00047265e-02 -7.68099248e-01 -1.89622253e-01\n",
      "  1.45644569e+00 -1.01471508e+00 -1.62530452e-01  2.18984699e+00\n",
      "  9.65666994e-02  9.71695244e-01 -1.38055301e+00 -8.24034691e-01\n",
      "  4.19559240e-01  3.09390545e-01  4.33540970e-01  4.11357999e-01\n",
      " -7.77150095e-01 -9.26977694e-01 -3.57425928e-01  4.27528560e-01\n",
      "  5.64538598e-01 -6.54419780e-01 -3.11623454e-01 -7.99111068e-01\n",
      "  4.51580852e-01 -5.04209042e-01 -4.88851190e-01  1.10558040e-01\n",
      "  4.26796496e-01 -3.59203190e-01 -8.57571423e-01  1.07125640e+00\n",
      " -6.18961513e-01 -4.37407762e-01  6.69915140e-01  1.72120297e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "# First layer\n",
    "ann.add(tf.keras.layers.Dense(units = 4, activation = 'relu'))\n",
    "# Second layer\n",
    "ann.add(tf.keras.layers.Dense(units = 4, activation = 'relu'))\n",
    "# Output layer\n",
    "ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 920us/step - loss: 0.6971 - accuracy: 0.5296\n",
      "78/78 [==============================] - 0s 443us/step - loss: 0.6652 - accuracy: 0.6103\n",
      "13/13 [==============================] - 0s 613us/step - loss: 0.6902 - accuracy: 0.5343\n",
      "78/78 [==============================] - 0s 639us/step - loss: 0.6629 - accuracy: 0.6200\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.6512 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 473us/step - loss: 0.6443 - accuracy: 0.6594\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.6796 - accuracy: 0.5931\n",
      "78/78 [==============================] - 0s 741us/step - loss: 0.6429 - accuracy: 0.6525\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.6360 - accuracy: 0.6250WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 434us/step - loss: 0.6240 - accuracy: 0.6817\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.6783 - accuracy: 0.5833\n",
      "78/78 [==============================] - 0s 677us/step - loss: 0.6250 - accuracy: 0.6699\n",
      "78/78 [==============================] - 0s 460us/step - loss: 0.6052 - accuracy: 0.6918\n",
      "13/13 [==============================] - 0s 460us/step - loss: 0.6782 - accuracy: 0.6005\n",
      "78/78 [==============================] - 0s 626us/step - loss: 0.6086 - accuracy: 0.6817\n",
      "78/78 [==============================] - 0s 435us/step - loss: 0.5864 - accuracy: 0.7088\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.6823 - accuracy: 0.5833\n",
      "78/78 [==============================] - 0s 767us/step - loss: 0.5904 - accuracy: 0.6995\n",
      "78/78 [==============================] - 0s 460us/step - loss: 0.5681 - accuracy: 0.7194\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7629 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.6859 - accuracy: 0.5931\n",
      "78/78 [==============================] - 0s 664us/step - loss: 0.5738 - accuracy: 0.7040\n",
      "78/78 [==============================] - 0s 422us/step - loss: 0.5497 - accuracy: 0.7356\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.6918 - accuracy: 0.5760\n",
      "78/78 [==============================] - 0s 639us/step - loss: 0.5603 - accuracy: 0.7186\n",
      "78/78 [==============================] - 0s 434us/step - loss: 0.5350 - accuracy: 0.7413\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.6935 - accuracy: 0.5613\n",
      "78/78 [==============================] - 0s 639us/step - loss: 0.5445 - accuracy: 0.7303\n",
      "78/78 [==============================] - 0s 434us/step - loss: 0.5193 - accuracy: 0.7486\n",
      "13/13 [==============================] - 0s 614us/step - loss: 0.6925 - accuracy: 0.5809\n",
      "78/78 [==============================] - 0s 626us/step - loss: 0.5320 - accuracy: 0.7340\n",
      "78/78 [==============================] - 0s 434us/step - loss: 0.5060 - accuracy: 0.7579\n",
      "13/13 [==============================] - 0s 613us/step - loss: 0.6974 - accuracy: 0.5784\n",
      "78/78 [==============================] - 0s 677us/step - loss: 0.5187 - accuracy: 0.7413\n",
      "78/78 [==============================] - 0s 435us/step - loss: 0.4918 - accuracy: 0.7701\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.7043 - accuracy: 0.5931\n",
      "78/78 [==============================] - 0s 626us/step - loss: 0.5058 - accuracy: 0.7522\n",
      "78/78 [==============================] - 0s 422us/step - loss: 0.4797 - accuracy: 0.7782\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.7077 - accuracy: 0.5907\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.4922 - accuracy: 0.7812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 703us/step - loss: 0.4928 - accuracy: 0.7628\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.4695 - accuracy: 0.7188WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 447us/step - loss: 0.4680 - accuracy: 0.7924\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.7211 - accuracy: 0.5833\n",
      "78/78 [==============================] - 0s 703us/step - loss: 0.4847 - accuracy: 0.7701\n",
      "78/78 [==============================] - 0s 473us/step - loss: 0.4553 - accuracy: 0.7960\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7478 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.7231 - accuracy: 0.5784\n",
      "78/78 [==============================] - 0s 664us/step - loss: 0.4712 - accuracy: 0.7826\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.4318 - accuracy: 0.7812WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 498us/step - loss: 0.4422 - accuracy: 0.8045\n",
      "13/13 [==============================] - 0s 613us/step - loss: 0.7386 - accuracy: 0.5809\n",
      "78/78 [==============================] - 0s 690us/step - loss: 0.4601 - accuracy: 0.7871\n",
      "78/78 [==============================] - 0s 460us/step - loss: 0.4306 - accuracy: 0.8086\n",
      "13/13 [==============================] - 0s 613us/step - loss: 0.7623 - accuracy: 0.5907\n",
      "78/78 [==============================] - 0s 652us/step - loss: 0.4485 - accuracy: 0.7956\n",
      "78/78 [==============================] - 0s 422us/step - loss: 0.4232 - accuracy: 0.8118\n",
      "13/13 [==============================] - 0s 536us/step - loss: 0.7743 - accuracy: 0.5907\n",
      "78/78 [==============================] - 0s 728us/step - loss: 0.4407 - accuracy: 0.7981\n",
      "78/78 [==============================] - 0s 460us/step - loss: 0.4121 - accuracy: 0.8171\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.8093 - accuracy: 0.5784\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.3760 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 716us/step - loss: 0.4283 - accuracy: 0.8062\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.3749 - accuracy: 0.8438WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 626us/step - loss: 0.4014 - accuracy: 0.8228\n",
      "13/13 [==============================] - 0s 536us/step - loss: 0.8106 - accuracy: 0.5858\n",
      "78/78 [==============================] - 0s 639us/step - loss: 0.4263 - accuracy: 0.8009\n",
      "78/78 [==============================] - 0s 434us/step - loss: 0.3986 - accuracy: 0.8240\n",
      "13/13 [==============================] - 0s 613us/step - loss: 0.8161 - accuracy: 0.5907\n",
      "78/78 [==============================] - 0s 664us/step - loss: 0.4171 - accuracy: 0.8094\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.3810 - accuracy: 0.7812WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 473us/step - loss: 0.3892 - accuracy: 0.8289\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.8300 - accuracy: 0.5784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 639us/step - loss: 0.4070 - accuracy: 0.8163\n",
      "78/78 [==============================] - 0s 422us/step - loss: 0.3806 - accuracy: 0.8370\n",
      "13/13 [==============================] - 0s 613us/step - loss: 0.8356 - accuracy: 0.5882\n",
      "78/78 [==============================] - 0s 626us/step - loss: 0.3999 - accuracy: 0.8224\n",
      "78/78 [==============================] - 0s 422us/step - loss: 0.3733 - accuracy: 0.8370\n",
      "13/13 [==============================] - 0s 537us/step - loss: 0.8561 - accuracy: 0.5858\n",
      "78/78 [==============================] - 0s 665us/step - loss: 0.3866 - accuracy: 0.8244\n",
      "78/78 [==============================] - 0s 422us/step - loss: 0.3627 - accuracy: 0.8455\n",
      "13/13 [==============================] - 0s 614us/step - loss: 0.8721 - accuracy: 0.5858\n",
      "78/78 [==============================] - 0s 639us/step - loss: 0.3862 - accuracy: 0.8301\n",
      " 1/78 [..............................] - ETA: 0s - loss: 0.3117 - accuracy: 0.9062WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "78/78 [==============================] - 0s 524us/step - loss: 0.3568 - accuracy: 0.8504\n",
      "13/13 [==============================] - 0s 690us/step - loss: 0.9069 - accuracy: 0.5711\n"
     ]
    }
   ],
   "source": [
    "steps = []\n",
    "accs = []\n",
    "test_accs = []\n",
    "\n",
    "for i in range(0, 25):\n",
    "    ann.fit(scaled_X_train, y_train, epochs = 1)\n",
    "    [loss, accuracy] = ann.evaluate(scaled_X_train, y_train)\n",
    "    [loss_t, accuracy_t] = ann.evaluate(scaled_X_test, y_test)\n",
    "    accs.append(accuracy), steps.append(i), test_accs.append(accuracy_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x213de8ebb88>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAouUlEQVR4nO3dd3yV5f3/8deVCRkQQiZhJOyAKMhUVHAhOKpUa3G1alu16retfrW1dv6+1dZa7XQgdbTaWm3dA0FFFFSooOwRtkkgZBAI2eOc6/fHFSRExgk5yck55/18PM4jZ9w553PnJO/c57qvYay1iIhI6IkIdAEiItIxFPAiIiFKAS8iEqIU8CIiIUoBLyISoqIC9cIpKSk2Ozs7UC8vIhKUPv300zJrbaov2wYs4LOzs1m+fHmgXl5EJCgZYz73dVs10YiIhCgFvIhIiFLAi4iEKAW8iEiIUsCLiIQoBbyISIhSwIuIhKiA9YMXEQkX1lpKK+tZX7Sf9UX7OTEridOGpHT46yrgRUT8qMnjZVtZNet37WdDc6BvKNpPWVXDF9t8d+ogBbyISEdr9Hgp3l/Hrn117KmqxxiIjIggKsIQEWHcV2OIijRERhgijfsaFemul1c3tAjySvKKK2lo8gIQExnB0IwEzhqeRm5mD0Zk9mB4Zg96do/ulH1TwItIyPJ6LWXV9RTtq6Ooopad++oo2ldLUUUduypq2bWvlpLKevyxsF1yfAwjMntw7anZjMjsQW5mDwamxhMdGbhTnQp4EQl6DU1etpdVs6m4kk3FleTtrmRzSRU799bS4PEesm236Aj69OxOZlI3Th+SSp+k7vTp2Y3MpO6kJsRiDHi8liavxdN8afJ68Xqhyev94r4D2yR0i2JEZg/SEmMxxgToJ3B4CngRCRoer6WgvIa84ko27XbNIZuKK9lWWk2T1x2GR0YYclLiyc1MZNrIdLKSupPZszuZPbvRJ6k7veKiu1wQdxQFvIh0WdZa1uys4JUVu/hkxx62lFRR13jwiLxfcneGpSdyTm46wzISGZqeyMDUeGKjIgNYddehgBeRLid/Tw2vrNzJKyt3sq20mpjICCbkJHP1xAEMTU9kaEYiQ9ISiI9VhB2Nfjoi0iXsqarnzTVFvLJiJ5/l7wNgYk4yN5w+kBknZNIzrnN6noQSBbyIBExtg4d3NhTzyoqdLNpUSpPXMjwjkR9NH85XRvchK6l7oEsMagp4EelwTR4vNY0eahs81DR4+HxPNa+t2sX8tbupbvCQ2bMb3zo9h0tGZ5Gb2SPQ5YYMBbyIHJeGJi8fbS3jvQ0llFXVU9PQHOCNTQevN39t3VURILFbFBed1IeLR2cxMSeZiIjw6NnSmRTwIuKzukYPH24uY+7aIt5ZX0xlXRMJsVFk9uxGXEwk3WMiSUvsRveYSOKiI5vviyIuJvKLx+NiIkmOj2ViTjLdotXbpSMp4EXkqOoaPXywqZS5a4pYsKGEqvomenSL4ryRGZw/KoPJg1PULbGLUsCLyJfUNDTxfp4L9fc2llDT4CEpLpoLRmUyY1QGpw5KISZKs413dQp4EQGgoqaR9zeVMG/tbhbmlVDX6KV3fAyXjMni/BMymTgwOaDzqkjbKeBFwtjW0ioWbChmwYYSln++F4/XkpIQy9fG9mPGqAwmZCcTpVAPWgp4kTDS6PGyfMdeF+obS9heVg3A8IxEbpoykLNz0xndN0k9WkKETwFvjJkO/AmIBB631t7X6vFewJPAIKAOuN5au9bPtYrIcTjQ9PLuhhI+yCthf10TMZERTBrUm+smZ3PW8DT69ooLdJnSAY4Z8MaYSOBh4FygEFhmjHnNWru+xWZ3AyuttTONMcObtz+7IwoWkWMrqazjrTW7mbumqEXTSwznjczg7Nx0ThuSQoLmcQl5vrzDE4At1tptAMaY54CLgZYBPwL4DYC1dqMxJtsYk26tLfZ3wSJyeHurG5i3bjevr9rF0m178FoYlq6ml3DmS8BnAQUtbhcCE1ttswr4KvChMWYCMADoCxwS8MaYG4AbAPr373+cJYvIAZV1jby9rpg3Vu9i8eYymryWgSnx3HrWEC46MZMh6YmBLlECyJeAP9y//NYLXN0H/MkYsxJYA6wAmr70TdbOAeYAjBs3zg+LZImEn5qGJt7bWMLrq3axMK+UhiYvWUnd+fbpA7nwxExG9ukRNgtayNH5EvCFQL8Wt/sCu1puYK3dD1wHYNxv1vbmi4i0sqm4khc+LWTn3lpioiKIjYogJiqCmMgIYqMjiImMPPT+5utea3lvYynvri+mttFDWmIsV03sz0Un9WFMvySFunyJLwG/DBhijMkBdgKzgCtbbmCMSQJqrLUNwLeBRc2hLyK4ppQ3Vhfx/LICVhbsIzrS0C85jkaPl4YmL/VNB796vEf+cJscH8NXT87iopP6MD47mUi1qctRHDPgrbVNxphbgfm4bpJPWmvXGWNuan58NpALPG2M8eBOvn6rA2sWCQrWWpbt2MvzywqYu6aI2kYPQ9MT+OkFucwck0XvhNjDfp/Ha5vD3vNF6B8I/oGp8RpNKj7zqZ+UtXYuMLfVfbNbXF8CDPFvaSLBqWR/HS98Vsh/lheyvayahNgoLhmTxeXj+jLah6aUyAhD9+aZF0XaQx1hRfyg0ePlvY0l/HtZAe9vKsXjtUzITuaWMwdz/qgM4mL0pyadT791Iu1QuLeGZ/+bz7+XF1JWVU9aYiw3nDGQr43ty8DUhECXJ2FOAS/SRl6vZdHmUv6x9HMWbCzBAGcNT2fW+H5MHZaqybmky1DAi/hob3UD//m0gH8szSe/vIaUhBhumTqYKyb21+LQ0iUp4EWOwlrLqsIKnlnyOa+v3kVDk5cJ2cnccd4wpo/M0KIX0qUp4EUOo7bBw+urdvHM0s9Zs7OC+JhILh/Xl6snDWB4Ro9AlyfiEwW8SAullfXMWbSV55cVsL+uiaHpCfzq4pFcMiaLxG7RgS5PpE0U8CJAdX0Tf128jb8u2kZdk5cZJ2RwzaQBTMhJ1hQAErQU8BLWGj1enltWwJ/e3UxZVT3nj8rgjmnD1MVRQoICXsKStZZ5a3fzu/l5bCurZkJOMn/9xljG9O8V6NJE/EYBL2Hnk+3l/OatDazI38eQtAQe/8Y4zs5NU1OMhBwFvISNzcWV/HbeRt7dUEJ6j1h+e+koLj25rwYmSchSwEvI211Rxx/e2cR/Pi0gPiaKO88bxvWTczSZl4Q8BbyEHGst28uqWVmwj+Wf7+WlzwrxeC3XnprDrWcNJjk+JtAlinQKBbwEvfLqBlYV7GNFwT5WFuxjVcE+KmobAYiLiWTGCZncfu5Q+iXHBbhSkc6lgJegUtfoYX3RflbmuzBfWbCP/PIaACIMDE1PZMYJGYzul8To/kkMSUvUqkcSthTwEhTy99Twh3c38cbqXTR63JJ2GT26MbpfEldO7M9JfZM4sW9P4mP1Ky1ygP4apEsr2V/HX97bwr8+yScq0nDlhP6cMqg3o/v1IqNnt0CXJ9KlKeClS6qobeSxD7by1Ec7aPR4+fr4fnzv7CGk91Coi/hKAS9dSm2Dh6c+3s7s97eyv66Ji0f34bZzhpKdEh/o0kSCjgJeuoQDc8L8ZcFmSirrOWt4GndMG8aIPpqaV+R4KeAloLxey+urd/Hg25vIL69hfHYvHr7qZMZnJwe6NJGgp4CXgPB6LQvzSvjd/Dw27q4kN7MHT107nqnDUjUnjIifKOClU20vq+alzwp56bOd7NxXy4Decfxp1mguOrEPEeqvLuJXCnjpcBW1jby5uogXPyvk08/3EmHgtCGp/HD6MM4flUm0JvsS6RAKeOkQTR4vi7eU8eKnhby9vpiGJi9D0hK4a8ZwZo7JUndHkU6ggBe/yttdyYufFfLKip2UVNaTFBfNFeP7cenYvozK6qn2dZFOpICX42atZVdFHZt2V7K+aD/z1u5mzc4KoiIMU4elcdnYLM4cnkZslKblFQkEBbz4pKyqnk27K8krrmRTcSV5uyvZVFxFVX3TF9uM7NODn184gotH96F3QmwAqxURUMDLYZRXN/DW2qIWgV5FeXXDF4/3iotmWEYil56cxdCMRIalJzIkPZGe3aMDWLWItKaAl0Psrqhj1pwl7NhTQ3xMJEMzEpk2Ip2h6YkMy0hkaHoiKQkxaksXCQIKePnCgXAvq2rgX9+ZxKSByQpykSCmgBfg0HD/+/UTGDugV6BLEpF20ggTUbiLhCgFfJhTuIuELgV8GFO4i4Q2nwLeGDPdGJNnjNlijLnrMI/3NMa8boxZZYxZZ4y5zv+lij8p3EVC3zED3hgTCTwMzABGAFcYY0a02uwWYL219iRgKvCgMSbGz7WKnyjcRcKDL0fwE4At1tpt1toG4Dng4lbbWCDRuD51CUA50IR0OQp3kfDhS8BnAQUtbhc239fSQ0AusAtYA3zfWutt/UTGmBuMMcuNMctLS0uPs2Q5Xgp3kfDiS8AfbqSLbXX7PGAl0AcYDTxkjPnSYprW2jnW2nHW2nGpqaltLFXaQ+EuEn58CfhCoF+L231xR+otXQe8ZJ0twHZguH9KlPZSuIuEJ19Gsi4DhhhjcoCdwCzgylbb5ANnA4uNMenAMGCbPwuVtrHWsqWkiiXb9vDEh9vZo3AXCTvHDHhrbZMx5lZgPhAJPGmtXWeMuan58dnAr4C/GWPW4Jp0fmStLevAuqUVay0F5bV8vLWMj7fu4eOteyirqgdgQO84hbtIGPJpLhpr7Vxgbqv7Zre4vguY5t/S5FiKKmpZ0hzmS7buYee+WgBSE2OZPLg3pw7qzamDUuiXHBfgSkUkEDTZWBCx1rIwr4QFG0pYsnUP28qqAUiKi+aUgb25acpAThmUwqDUeM0CKSIK+GDx0ZYy7p+3kVWFFSTERjExJ5krJ/bnlEG9yc3oQUSEAl1EDqWA7+JWFuzjd/M38tGWPWQldef+y05k5pgsoiM1jZCIHJ0CvovaXFzJA2/nMX9dMcnxMfz8whFcNam/FrAWEZ8p4LuYwr01/PHdzbz0WSFxMVHcds5QvnV6DgmxeqtEpG2UGl1EWVU9Dy/cwj+X5oOB6yfncPOZg0mO15xtInJ8FPABtr+ukccXbeOJD7dT2+jh8nH9+N7ZQ+iT1D3QpYlIkFPAB9C8tUX8+KU17K1p5IJRmdw+bSiDUhMCXZaIhAgFfICsKazge8+tJDcjkaevH8Wovj0DXZKIhBgFfADsqarnxmeWk5oQy5PXjqd3QmygSxKREKSA72RNHi+3PruCsuoGXrzpVIW7iHQYjZbpZPe9tZEl2/bwm5lqlhGRjqWA70SvrtzJ4x9u59pTs7l0bN9AlyMiIU4B30nW79rPj15czYTsZH5yQW6gyxGRMKCA7wR7qxu48R/LSeoew8NXnax5ZESkU+gkawfzeC3fe24FxRX1PH/jJFITdVJVRDqHAr6DPfB2Hos3l3HfV0cxpr9WVBKRzqO2gg705uoiHn1/K1dM6M+sCf0DXY6IhBkFfAfJ213JnS+sYkz/JH75lRGBLkdEwpACvgNU1DZy4zPLiY+NYvbVYzWHu4gEhALez7xeyw+eW0Hh3loeuepk0nt0C3RJIhKmFPB+9scFm1mYV8ovLhrB+OzkQJcjImFMAe9Hb6/bzZ8XbOaysX25etKAQJcjImFOAe8n+XtquP3fqzixb0/uueQEjDGBLklEwpwC3k/um7cBr7U8evVYukXrpKqIBJ4C3g8+/bycuWt2c+MZg8jSUnsi0kUo4NvJWss9b24gvUcs3zkjJ9DliIh8QQHfTm+uKWJF/j7+d9ow4mI084OIdB0K+Haob/Lw23kbGZ6RyKUna353EelaFPDt8MySzykor+UnF+QSGaFeMyLStSjgj9O+mgb+vGAzU4amcvqQ1ECXIyLyJQr44/TnBVuoqm/i7vO1OpOIdE0K+OOwo6yaZ5bu4Ovj+zEsIzHQ5YiIHJYC/jjcP38j0ZER3HbO0ECXIiJyRAr4Nmo5qClNM0WKSBfmU8AbY6YbY/KMMVuMMXcd5vE7jTErmy9rjTEeY0zITaV4YFBTWqIGNYlI13fMgDfGRAIPAzOAEcAVxphDliiy1v7OWjvaWjsa+DHwgbW2vAPqDagDg5ru0KAmEQkCvhzBTwC2WGu3WWsbgOeAi4+y/RXAv/xRXFdyyKCmsRrUJCJdny8BnwUUtLhd2Hzflxhj4oDpwIvtL61r0aAmEQk2vgT84dLMHmHbi4CPjtQ8Y4y5wRiz3BizvLS01NcaA06DmkQkGPkS8IVAvxa3+wK7jrDtLI7SPGOtnWOtHWetHZeaGjxB+Zf3NKhJRIKPLwG/DBhijMkxxsTgQvy11hsZY3oCU4BX/VtiYO0oq+bpJTu4fJwGNYlIcDlmVxBrbZMx5lZgPhAJPGmtXWeMuan58dnNm84E3rbWVndYtQFwYFDT7edqUJOIBBef+vpZa+cCc1vdN7vV7b8Bf/NXYV3BgUFNt50zVIOaRCToaCTrEWhQk4gEOwX8Ecxds1uDmkQkqCngD0ODmkQkFCjgD+PvH+8gv7xGg5pEJKgp4FvZU1XPXxZs4azhaRrUJCJBTQHfyh/f3UxNo4e7zx8e6FJERNpFAd/C5uJKnv0kn6sn9mdwmgY1iUhwU8C3cO/cDcTFRPJ9rdQkIiFAAd/sg02lvJ9XyvfPHkJyfEygyxERaTcFPNDk8XLvm+sZ0DuOa04ZEOhyRET8QgEPPLesgE3FVfx4Ri6xUZGBLkdExC/CPuD31zXyh3c2MTEnmfNGpge6HBERvwn7gH944RbKaxr42YUjMEaDmkQkdIR1wOfvqeGpD3dw6cl9OSGrZ6DLERHxq7AO+N/O20hkhOHO84YFuhQREb8L24BftqOcN9cUcdOUQaRrrncRCUFhGfBer+WeN9aT0aOb5noXkZAVlgH/6qqdrCqs4IfTNde7iISusAv42gYP98/L48S+PblkdFagyxER6TBhF/B/XbyNooo6fnrBCCI017uIhLCwCvji/XU8+v5WZpyQwYSc5ECXIyLSocIq4B+Yn4fHa7lrhuZ6F5HQFzYBv3ZnBS98Vsi1k7MZ0Ds+0OWIiHS4sAh4ay33vLmeXnEx3HLm4ECXIyLSKcIi4N9ZX8zSbeXcds4QenaPDnQ5IiKdIiwC/okPt5OTEs8VE/oHuhQRkU4T8gFfVlXPsh3lfOWkPkRFhvzuioh8IeQT7931xXgtnDcyI9CliIh0qpAP+HnrdtM/OY7czMRAlyIi0qlCOuD31zXy8ZY9TD8hQ4t5iEjYCemAX7ixhAaPV80zIhKWQjrg56/bTVpiLGP6JQW6FBGRTheyAV/X6GHhxlKmjUzXpGIiEpZCNuAXbSqlttHD9JGZgS5FRCQgQjbg568rpmf3aCYO1KyRIhKeQjLgGz1e3t1QzDm56URrcJOIhCmf0s8YM90Yk2eM2WKMuesI20w1xqw0xqwzxnzg3zLb5r/byqmobWT6Ceo9IyLh65gLkhpjIoGHgXOBQmCZMeY1a+36FtskAY8A0621+caYtA6q1yfz1hURFxPJ6UNSAlmG/1gLZZtg+yLIXwJpuTDxJojV4C0ROTJfVpyeAGyx1m4DMMY8B1wMrG+xzZXAS9bafABrbYm/C/WV12uZv66YqcNS6RYdGagy2m9fPmz7wIX69kVQtdvdn5AOa1+EpY/CabfB+G9DdPfA1ioiXZIvAZ8FFLS4XQhMbLXNUCDaGPM+kAj8yVr7dOsnMsbcANwA0L9/x8zsuKJgL6WV9cE3uKmqpDnMm0N97w53f3wa5Jxx8JKcA4Wfwnu/grd/Ch8/BGfcASd/E6JiAroLItK1+BLwh+tEbg/zPGOBs4HuwBJjzFJr7aZDvsnaOcAcgHHjxrV+Dr+Yv66YmMgIzhoe0Faio2uqh7LNULIBCpe5QC/d4B6L7QnZp8HE78LAKZA6HFpPs9B3LHzjFdjxISz4Fcy9Az7+M0y5C078OkT68raKSKjzJQkKgX4tbvcFdh1mmzJrbTVQbYxZBJwEbKITWWuZt3Y3kwf3JrFbF1jYw+txR+Il612YH/i6Zwt4m9w2Ud1hwClw0tfdEXrmaIjwsWkp+zS4fh5sWeCO6F+9GT78A5x5N4y4BCK6cA8iT5P7tDJgMkR3C3Q1IiHJl4BfBgwxxuQAO4FZuDb3ll4FHjLGRAExuCacP/izUF9sKKokv7yGW84c1NkvDZ5G2LEYite5EC9eB6V50FR7cJte2ZA2AoZf4L6mjYDeg9vXtGIMDDkHBp8NG9+A9+6FF66D9N/DWT+BodO//AngSBprobrUXbolQe8O+jnu2Qov3+g+vfQZA5c/DUlajEXE344Z8NbaJmPMrcB8IBJ40lq7zhhzU/Pjs621G4wx84DVgBd43Fq7tiMLP5x563YTYeCc3PTOe9HqMlj+FCx/AiqL3H0JGa6ny7jrIX2Eu54yDGITOq4OYyD3Ihh2vjsJu/DX8K9ZkDXOtdHHxDeHd5lr7z9w/UCgV5dBQ2XLJ4Sx34SzfwFxfhosZq37Ob39M4iMgTN+CP+dDY+dAZc+DoPP8c/riAgAxtoOaQo/pnHjxtnly5f79TnP+8MikuKief7GU/z6vIdVtNqF05oXwFMPg85yPVr6n+K/QGwPTyOsfBY+uB/2Fx76mImAuBSIT4WEVPc1PhXiUw5e377Y7V+3nnDu/4PRV7evyWd/Ebx6C2xd4H5WFz8MPfq4o/nnr3HNV2feDaff0bWblkQCzBjzqbV2nE/bhkrAby+r5swH3ucXF43gusk5fnveQ3iaIO9N+O9j8PlHEB0HJ10BE2+E1GEd85rt1VjnTuJGdz8Y3t17+Raixevgzf91fe/7jocLHoTMk9pew9oX4Y3b3cnlab9y/whbNhs11MAbP4DVz8OQ8+Crj7kaReRL2hLwIdPdYv4610+8Q7pH1pTDZ0/DssehosC1F0+7B8Zc3fWDKLobDJ12fN+bPhKuewtWPQfv/AzmTHXhfOZPoHvSsb+/di+8eQesfQGyxsLMOZAy+MvbxcTBzMfcP5F5P4bHpsDXnzm+fyYi8oWQOYK/5OGPsNby6q2n+e05KdngmilWPe9Olmaf7kaQDpvhe0+XUFG7D967x7Whx6W4I/ETv37kE7hb34NXboHqEpjyIzjtdt+6bxYsg/98050TuPD37p+oiHyhLUfwIdHYWVRRy8qCfUzz19G71+PahR+Z5I5eR10GN30I174BuReGX7iDO2K/4AH4zkL3CeblG+FvF0Dx+kO3a6iBuXfCMzPdSeVvvwtTfuh73/x+4+HGRdB/kmuzf+17rplJRNosJJpo3l5XDOC/ycWWPQEbXnNTAZzyPxDf2z/PGwr6jIZvvQMrnoF3fwGzT4NJ34Wpd7n5cl66wfXzn3QznP3z45tGIT4FrnkZFt4Lix+EolWuK2WvAX7fHZFQFhJNNFfMWUpZVT3v3D6l/U9WWQwPjYO+4+Dql3zvQx6Oasrh3V+68xPxKe52YiZc8ogbhesPG+fCyze5k8Jffdz1+T+chuovd/2sKnG3E9IgZ4pr09coXwlyYXWStby6gU92lHPzVD8Nynn7p9BUB+c/oHA/lrhk+Mqf4eRvuJ/bkIFw3q99OwHrq+Hnww0L4d/fgH9eBqOvcu/LF/33m0O9sebw3x+TAA1V7npsDzf698C8Pmkj9B5LSAv6gH93QzEer/VP75nti2DNv91JwY4axRmK+o5zUyZ0lN6DXLPQ3Dtdj5zuvQ722e89uEU//tb9+VNcE1FViRtlvH2Rm6Ezb6573riUVhO5DeycwK/d57qeJg1wA+FEOkjQN9F862/LyCuuZPEPz8S054+zqQFmTwZPA9y8VFPwhrJ9+W4g14GZOw+MQO7ZzwV99unuXEPvwRDphzmNGqohf+nB2UKLVoH1gol0o4zPuNM/ryNhIWyaaKrqm1i8uYxrThnQvnAHWPKQO0l41QsK91CX1B/GXOUu1rqTwtvedwGcNxdW/tNtFxENKUPdVBNpuW5cQFou9Ox/9IFiTQ2wc/nBufwLPgFvo3u+vuPdFA0DToVV/4IPfgub5sNX53TdwXIStII64BduLKHB421/75l9+W5If+5FMORc/xQnwcEYSBniLhO+A15v86yf6w9OHFfwiWsaOiA6HtKGH5wwLi3Xra6148ODq2411gDGndg95Wb3yaD/KW5OoAMGTnFjKl7/gZuP55xfwoQbNVWD+E1QB/z8dbtJSYjl5P7tHE361l1ufpbp9/mnMAleERGQcYK7tFS3H0o3Hjr1c95brrtoS6nD3eCsnCmQPfnYI51HXAz9JsFr/wPz7nLPeckj0LOvf/dLwlLQBnxdo4eFG0u4eEwWkRHtaJ7Je8vNL3Pu/+mPSo6sWw/oN8FdWqoqdWFft88FdeJxzGSamA5XPg+f/R3m3Q2PnArn/w5OvFy9fKRdgjbgP9pSRnWDh+nt6T3TUANzf+iOuibd7L/iJHwkpEKCH/r8GwNjr3VNOS9/F16+wR14XPCHjhto52mC8q1fXpCmzxg322dUbMe8rnSaoA34eWt3k9gtikkD2/HLv/gBqMiHa+eqF4N0DckD4bq58NGf3Jz++UvhKw8d/4Rx4M4rVBQcGuIl612nAk+D28ZEQPIg6JUDa/7jDn4u/7v+LoJcUAZ8k8fLOxuKOSc3nZio4zwhVboJPvqzm+43e7J/CxRpj4hIOP12twDKyzfCs19zR/fT7j100Rhroa6i1aCvVqN5K3a6cwcHBnuB6w6alutWAUtr7hmUMvTg0omf/NWt8/vSDW4hlnCceylEBGXAf7K9nH01jcc/uMlamPu/bprac//Pv8WJ+EvmiXDD+24Wz4//AlsXur75LUPc23j47+2e7AZ7Jaa70b8HunmmDnOLuBzNhO+45Rvf+Zlrprn4kc7p2eP1uimmv/hHVXLoP6vImOYuqyNcs6o/R0yHqKAM+PnrdtMtOoIpQ1OP7wnWvui6s13woJunRKSriop1UzMPne4md6vb51bCyjzxMCN4my9xye1vWpn8PTdlx8J7XQ0X/tF/J3zLtsCnT0Hl7kP/WdWUuQFgrR1Ygayx9tBlJXtkHeymeuBr6jCNY2kh6ALe67XMX1fM1KFpdI85jo+OdRUw/253Imnsdf4vUKQjZE92Uy93pjPudKH64e8hqpvrRtyekLfWLZrz9s9ckPfMcv+QknPcNNFfmmoi7dAVyKyFisLmcwjrDp5L2L7ILZsJzecSBjaHfa7r/dQWMQmu91LL8QpBLOgCflXhPnbvr+O8E45zYe2Fv3Zzk1zxnNoWRY7GGDflc1MdLH3Ehfw5vzy+kN+/C165GbYtdOcWvvIQ9Mhsez1J/dyl5UlnTxOUb2vVG2g9bHzz8J8IjmXRAzD9N27gY5B3Uw26gK9t8HBSvyTOGn4cAV+0Cj6ZA+O/BVkn+784kVBjjJshtKkePvqja/6YelfbnmPNC/Dm7W4h+At+D+Ou929wRkZB6lB3GXnJwfub6g/2EvJV0Wp464fw72tg0NluPEIQTzwY9JON+czrhSfOhX2fw63Luv5aqiJdidfrRtuu/Aec8//gtB8c+3tqyt2i7etecnPwzHwsOMLS0wTL/grv3euafib/wPVq6iJt+2Ez2VibrHjaTQA18zGFu0hbRUS4uf+b6tzJ3qhuMOmmI2+/+V235GJNGZz1U5h8W/AsthIZ5VYpGznTrXOw6H5Y/RzMuN/NHdRe9VXuk0Vccvuf6xjCY1aj6jJ45xcwYLJbKFpE2i4iEmbOhuEXwrwfwfKnvrxNQzW8cTv881LXjfHbC5qnQw6ScG8pMcONA/jm6xDVHf41C56dBXt3tO15murdRHTv3QtPnAe/HQBLH+2QklsLwp96G3ma4PXvu4EeFzwY9CdNRAIqMhouewqevwreuM01W5w0yz1WsMwNzCrfBqfcCmf97ODgqWCWcwbc9CH891F4/7fw8EQ4/Q7XlfRw0zl4mtz5vu0fuEv+UvfJx0S43nun/o/r9toJQrsN3uuBV74Lq593XbwmfbdjX08kXDTWwbOXu5WyZj7mpj1Y/KDrm37Jo5BzeqAr7BgVO2H+j2H9q6475vm/cydjD3TX3L7IHa3X73fbp408uGJY9uRjDzLzQVva4EM34L1eeOP7bkHos37qPiaKiP80VMM/LoP8j93tk66EGff5JcS6vC0L3BKS5Vvd/tZVuPt75bh5/g+sDNYBAyl1ktVa10b42dPuo5TCXcT/YuLdNMfv/NzNa5N7UaAr6jyDz4abl7i29LJN7vxezulutbAuJPQC3lo3h8Ync5rbAX8a6IpEQle3HnDRHwNdRWBExfrWXTSAQq8XzcJfu4mZxn8bpt2jk6oiErZCK+AXP+j6rI65Bmb8TuEuImEtdAJ+ySOw4P9g1Nfgoj9p4WIRCXuhkYLLnnBdl3K/ApfM1iRiIiKEQsCv+KebyGjodLj0ieAcMSci0gGCO+DXvACv3QoDz4Sv/R2iYgJdkYhIlxG8Ab/hdbdmZP9TYNazoTEkWkTEj3wKeGPMdGNMnjFmizHmS5NBG2OmGmMqjDErmy8/93+pLWx6G/5znZvT/crn3dqqIiJyiGM2WBtjIoGHgXOBQmCZMeY1a+36VpsuttZe2AE1HmrbB/D81ZA+Aq56AWITO/wlRUSCkS9H8BOALdbabdbaBuA54OKOLesoEjMg+zS45hWtqi4ichS+BHwWUNDidmHzfa2dYoxZZYx5yxgz8nBPZIy5wRiz3BizvLS09DjKxa2afs1LnTJZvohIMPMl4A83HLT1FJSfAQOstScBfwFeOdwTWWvnWGvHWWvHpaamtqlQERFpG18CvhDo1+J2X2BXyw2stfuttVXN1+cC0caYFL9VKSIibeZLwC8DhhhjcowxMcAs4LWWGxhjMoxxE78YYyY0P+8efxcrIiK+O2YvGmttkzHmVmA+EAk8aa1dZ4y5qfnx2cBlwHeNMU1ALTDLBmolERERAUJ5RScRkRDUlhWdgnckq4iIHJUCXkQkRCngRURCVMDa4I0xpcDnx/ntKUCZH8sJNuG8/+G87xDe+699dwZYa30aSBSwgG8PY8xyX08yhKJw3v9w3ncI7/3Xvrd939VEIyISohTwIiIhKlgDfk6gCwiwcN7/cN53CO/91763UVC2wYuIyLEF6xG8iIgcgwJeRCREBV3AH2t92FBmjNlhjFnTvO5tyE/kY4x50hhTYoxZ2+K+ZGPMO8aYzc1fewWyxo5yhH3/pTFmZ4u1j88PZI0dxRjTzxiz0BizwRizzhjz/eb7w+W9P9L+t/n9D6o2+Ob1YTfRYn1Y4IrDrA8bkowxO4Bx1tqwGOxhjDkDqAKettae0Hzf/UC5tfa+5n/wvay1PwpknR3hCPv+S6DKWvtAIGvraMaYTCDTWvuZMSYR+BS4BLiW8Hjvj7T/l9PG9z/YjuC71vqw0qGstYuA8lZ3Xwz8vfn633G/+CHnCPseFqy1Rdbaz5qvVwIbcMuEhst7f6T9b7NgC3hf14cNVRZ42xjzqTHmhkAXEyDp1toicH8IQFqA6+lstxpjVjc34YRkE0VLxphsYAzwX8LwvW+1/9DG9z/YAt6X9WFD2WRr7cnADOCW5o/xEj4eBQYBo4Ei4MGAVtPBjDEJwIvAD6y1+wNdT2c7zP63+f0PtoA/5vqwocxau6v5awnwMq7JKtwUN7dRHmirLAlwPZ3GWltsrfVYa73AXwnh998YE40Lt39aa19qvjts3vvD7f/xvP/BFvDHXB82VBlj4ptPuGCMiQemAWuP/l0h6TXgm83Xvwm8GsBaOtWBcGs2kxB9/5vXd34C2GCt/X2Lh8LivT/S/h/P+x9UvWgAmrsG/ZGD68PeG9iKOocxZiDuqB3cWrrPhvq+G2P+BUzFTZVaDPwCeAX4N9AfyAe+Zq0NuZORR9j3qbiP5xbYAdx4oE06lBhjTgMWA2sAb/Pdd+PaocPhvT/S/l9BG9//oAt4ERHxTbA10YiIiI8U8CIiIUoBLyISohTwIiIhSgEvIhKiFPAiIiFKAS8iEqL+P9JrpB7PGn9jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(steps, accs)\n",
    "plt.plot(steps, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ann.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.850284  ],\n",
       "       [0.02492696],\n",
       "       [0.07189468],\n",
       "       [0.07804167],\n",
       "       [0.2935543 ],\n",
       "       [0.13958934],\n",
       "       [0.14376986],\n",
       "       [0.24368817],\n",
       "       [0.62740684],\n",
       "       [0.05425075],\n",
       "       [0.27551737],\n",
       "       [0.76272595],\n",
       "       [0.14329562],\n",
       "       [0.31249043],\n",
       "       [0.05942544],\n",
       "       [0.01615885],\n",
       "       [0.03868657],\n",
       "       [0.30838048],\n",
       "       [0.1976479 ],\n",
       "       [0.03546044],\n",
       "       [0.26803026],\n",
       "       [0.01279387],\n",
       "       [0.90836227],\n",
       "       [0.98998845],\n",
       "       [0.02410406],\n",
       "       [0.37276852],\n",
       "       [0.27025953],\n",
       "       [0.10794804],\n",
       "       [0.01830181],\n",
       "       [0.9591403 ],\n",
       "       [0.62562585],\n",
       "       [0.29186636],\n",
       "       [0.79327726],\n",
       "       [0.22967914],\n",
       "       [0.2172341 ],\n",
       "       [0.95091015],\n",
       "       [0.99989974],\n",
       "       [0.99819255],\n",
       "       [0.9321914 ],\n",
       "       [0.15112975],\n",
       "       [0.35260832],\n",
       "       [0.999941  ],\n",
       "       [0.4011478 ],\n",
       "       [0.984572  ],\n",
       "       [0.9999987 ],\n",
       "       [0.9994031 ],\n",
       "       [0.99938124],\n",
       "       [0.11299291],\n",
       "       [0.99427646],\n",
       "       [0.9687673 ],\n",
       "       [0.9994327 ],\n",
       "       [0.96721905],\n",
       "       [0.9024304 ],\n",
       "       [0.98624563],\n",
       "       [0.9995645 ],\n",
       "       [0.97836065],\n",
       "       [0.99948937],\n",
       "       [0.92404836],\n",
       "       [0.9923079 ],\n",
       "       [0.92936444],\n",
       "       [0.99920905],\n",
       "       [0.9998714 ],\n",
       "       [0.99967575],\n",
       "       [0.99907136],\n",
       "       [0.9730759 ],\n",
       "       [0.98743886],\n",
       "       [0.92052364],\n",
       "       [0.83207417],\n",
       "       [0.9007604 ],\n",
       "       [0.32971403],\n",
       "       [0.9466508 ],\n",
       "       [0.16491151],\n",
       "       [0.11421731],\n",
       "       [0.32336363],\n",
       "       [0.4797277 ],\n",
       "       [0.9348074 ],\n",
       "       [0.574504  ],\n",
       "       [0.7016364 ],\n",
       "       [0.49198607],\n",
       "       [0.99856496],\n",
       "       [0.10279021],\n",
       "       [0.9900384 ],\n",
       "       [0.9957427 ],\n",
       "       [0.10149911],\n",
       "       [0.51420987],\n",
       "       [0.15197894],\n",
       "       [0.14029294],\n",
       "       [0.15616843],\n",
       "       [0.39487058],\n",
       "       [0.36158186],\n",
       "       [0.85129464],\n",
       "       [0.28801382],\n",
       "       [0.5297831 ],\n",
       "       [0.3679896 ],\n",
       "       [0.6455919 ],\n",
       "       [0.82824934],\n",
       "       [0.93040013],\n",
       "       [0.04244706],\n",
       "       [0.6002026 ],\n",
       "       [0.9786954 ],\n",
       "       [0.27280396],\n",
       "       [0.30831665],\n",
       "       [0.5772608 ],\n",
       "       [0.11027607],\n",
       "       [0.29020923],\n",
       "       [0.03610557],\n",
       "       [0.22836319],\n",
       "       [0.96261996],\n",
       "       [0.9921106 ],\n",
       "       [0.09966612],\n",
       "       [0.01645872],\n",
       "       [0.05170181],\n",
       "       [0.6510595 ],\n",
       "       [0.22829837],\n",
       "       [0.9411739 ],\n",
       "       [0.29996097],\n",
       "       [0.994316  ],\n",
       "       [0.87513757],\n",
       "       [0.14882007],\n",
       "       [0.07780838],\n",
       "       [0.9108448 ],\n",
       "       [0.17652547],\n",
       "       [0.25286397],\n",
       "       [0.9660665 ],\n",
       "       [0.9944744 ],\n",
       "       [0.95226777],\n",
       "       [0.9999744 ],\n",
       "       [0.9684217 ],\n",
       "       [0.5071718 ],\n",
       "       [0.9960937 ],\n",
       "       [0.9947285 ],\n",
       "       [0.980055  ],\n",
       "       [0.5142855 ],\n",
       "       [0.73932254],\n",
       "       [0.99486744],\n",
       "       [0.9324332 ],\n",
       "       [0.10278773],\n",
       "       [0.65782607],\n",
       "       [0.01266313],\n",
       "       [0.06228766],\n",
       "       [0.12207884],\n",
       "       [0.20260176],\n",
       "       [0.42019153],\n",
       "       [0.17934194],\n",
       "       [0.00490394],\n",
       "       [0.90411305],\n",
       "       [0.16687167],\n",
       "       [0.12565652],\n",
       "       [0.300601  ],\n",
       "       [0.0965865 ],\n",
       "       [0.9561217 ],\n",
       "       [0.7878798 ],\n",
       "       [0.04198876],\n",
       "       [0.9980292 ],\n",
       "       [0.09958601],\n",
       "       [0.2424677 ],\n",
       "       [0.35914433],\n",
       "       [0.9681282 ],\n",
       "       [0.957006  ],\n",
       "       [0.3372575 ],\n",
       "       [0.20740461],\n",
       "       [0.8111921 ],\n",
       "       [0.9197646 ],\n",
       "       [0.8075198 ],\n",
       "       [0.60820854],\n",
       "       [0.9996741 ],\n",
       "       [0.9748513 ],\n",
       "       [0.9288591 ],\n",
       "       [1.        ],\n",
       "       [0.946002  ],\n",
       "       [0.42538378],\n",
       "       [0.99596435],\n",
       "       [0.982063  ],\n",
       "       [0.96973026],\n",
       "       [0.34100708],\n",
       "       [0.9516304 ],\n",
       "       [0.5668201 ],\n",
       "       [0.22753116],\n",
       "       [0.99999774],\n",
       "       [0.14049134],\n",
       "       [0.6662794 ],\n",
       "       [0.13791779],\n",
       "       [0.88382995],\n",
       "       [0.51848024],\n",
       "       [0.9743498 ],\n",
       "       [0.9952296 ],\n",
       "       [0.89976096],\n",
       "       [0.7563418 ],\n",
       "       [0.8289296 ],\n",
       "       [0.637767  ],\n",
       "       [0.20730403],\n",
       "       [0.21863642],\n",
       "       [0.16451591],\n",
       "       [0.14178017],\n",
       "       [0.7471864 ],\n",
       "       [0.9890566 ],\n",
       "       [0.04212111],\n",
       "       [0.04441053],\n",
       "       [0.9259375 ],\n",
       "       [0.99949455],\n",
       "       [0.59750366],\n",
       "       [0.26056868],\n",
       "       [0.0715453 ],\n",
       "       [0.17817417],\n",
       "       [0.16442263],\n",
       "       [0.95133364],\n",
       "       [0.36321208],\n",
       "       [0.8148955 ],\n",
       "       [0.8255899 ],\n",
       "       [0.08174744],\n",
       "       [0.00903898],\n",
       "       [0.3128243 ],\n",
       "       [0.8170616 ],\n",
       "       [0.92787474],\n",
       "       [0.93621767],\n",
       "       [0.99998415],\n",
       "       [0.99659014],\n",
       "       [0.94640905],\n",
       "       [0.99923646],\n",
       "       [0.40038455],\n",
       "       [0.15800846],\n",
       "       [0.19074884],\n",
       "       [0.47868884],\n",
       "       [0.8668963 ],\n",
       "       [0.98298943],\n",
       "       [0.35634214],\n",
       "       [0.21412885],\n",
       "       [0.9982307 ],\n",
       "       [0.02542108],\n",
       "       [0.02909666],\n",
       "       [0.13958934],\n",
       "       [0.11209711],\n",
       "       [0.13958934],\n",
       "       [0.13958934],\n",
       "       [0.96834016],\n",
       "       [0.12265942],\n",
       "       [0.0661138 ],\n",
       "       [0.16746318],\n",
       "       [0.10341311],\n",
       "       [0.9656354 ],\n",
       "       [0.13928938],\n",
       "       [0.7871734 ],\n",
       "       [0.5085741 ],\n",
       "       [0.35738778],\n",
       "       [0.13958934],\n",
       "       [0.7676526 ],\n",
       "       [0.5774647 ],\n",
       "       [0.96438384],\n",
       "       [0.02271768],\n",
       "       [0.13958934],\n",
       "       [0.11692706],\n",
       "       [0.39395255],\n",
       "       [0.6957996 ],\n",
       "       [0.25646085],\n",
       "       [0.9834149 ],\n",
       "       [0.7289901 ],\n",
       "       [0.27826065],\n",
       "       [0.86207175],\n",
       "       [0.55994034],\n",
       "       [0.88906217],\n",
       "       [0.14927325],\n",
       "       [0.8462161 ],\n",
       "       [0.98666584],\n",
       "       [0.2527827 ],\n",
       "       [0.04529917],\n",
       "       [0.7109577 ],\n",
       "       [0.7477652 ],\n",
       "       [0.9979646 ],\n",
       "       [0.45030823],\n",
       "       [0.99883264],\n",
       "       [0.15245458],\n",
       "       [0.44513148],\n",
       "       [0.14616272],\n",
       "       [0.9987792 ],\n",
       "       [0.8678432 ],\n",
       "       [0.4786853 ],\n",
       "       [0.16688198],\n",
       "       [0.14212406],\n",
       "       [0.48552182],\n",
       "       [0.96852803],\n",
       "       [0.9132618 ],\n",
       "       [0.970811  ],\n",
       "       [0.13958934],\n",
       "       [0.99455065],\n",
       "       [0.9552282 ],\n",
       "       [0.9160659 ],\n",
       "       [0.99734616],\n",
       "       [0.11690393],\n",
       "       [0.03050187],\n",
       "       [0.5264843 ],\n",
       "       [0.62012285],\n",
       "       [0.26284018],\n",
       "       [0.99848765],\n",
       "       [0.97015333],\n",
       "       [0.07672271],\n",
       "       [0.3251167 ],\n",
       "       [0.9303645 ],\n",
       "       [0.37255952],\n",
       "       [0.09608421],\n",
       "       [0.05670264],\n",
       "       [0.6345397 ],\n",
       "       [0.7352433 ],\n",
       "       [0.577778  ],\n",
       "       [0.556687  ],\n",
       "       [0.28090763],\n",
       "       [0.84571016],\n",
       "       [0.9896759 ],\n",
       "       [0.9976244 ],\n",
       "       [0.07019994],\n",
       "       [0.97322   ],\n",
       "       [0.21914819],\n",
       "       [0.13588232],\n",
       "       [0.5494212 ],\n",
       "       [0.08326772],\n",
       "       [0.1504302 ],\n",
       "       [0.37419686],\n",
       "       [0.07815319],\n",
       "       [0.0283066 ],\n",
       "       [0.14523414],\n",
       "       [0.354585  ],\n",
       "       [0.6120641 ],\n",
       "       [0.9891283 ],\n",
       "       [0.98812276],\n",
       "       [0.20959914],\n",
       "       [0.05040121],\n",
       "       [0.59538245],\n",
       "       [0.16022485],\n",
       "       [0.24074882],\n",
       "       [0.5717904 ],\n",
       "       [0.18813825],\n",
       "       [0.0989711 ],\n",
       "       [0.16578636],\n",
       "       [0.6874267 ],\n",
       "       [0.20006812],\n",
       "       [0.9978861 ],\n",
       "       [0.99984455],\n",
       "       [0.98805726],\n",
       "       [0.10252705],\n",
       "       [0.2871834 ],\n",
       "       [0.3395604 ],\n",
       "       [0.9995279 ],\n",
       "       [0.37965664],\n",
       "       [0.4022498 ],\n",
       "       [0.36239016],\n",
       "       [0.4395942 ],\n",
       "       [0.06679079],\n",
       "       [0.95465976],\n",
       "       [0.32490942],\n",
       "       [0.21436253],\n",
       "       [0.10063407],\n",
       "       [0.1555208 ],\n",
       "       [0.03015667],\n",
       "       [0.15743983],\n",
       "       [0.9035578 ],\n",
       "       [0.9960905 ],\n",
       "       [0.11970133],\n",
       "       [0.01948979],\n",
       "       [0.90076363],\n",
       "       [0.9997567 ],\n",
       "       [0.9117253 ],\n",
       "       [0.8423921 ],\n",
       "       [0.10452884],\n",
       "       [0.5094161 ],\n",
       "       [0.38208616],\n",
       "       [0.2745366 ],\n",
       "       [0.02817959],\n",
       "       [0.45978492],\n",
       "       [0.97447515],\n",
       "       [0.27968138],\n",
       "       [0.16719162],\n",
       "       [0.06520101],\n",
       "       [0.9703233 ],\n",
       "       [0.28135857],\n",
       "       [0.14963624],\n",
       "       [0.01938257],\n",
       "       [0.02838165],\n",
       "       [0.4341115 ],\n",
       "       [0.13323337],\n",
       "       [0.01311654],\n",
       "       [0.7048872 ],\n",
       "       [0.0042094 ],\n",
       "       [0.68783224],\n",
       "       [0.05988011],\n",
       "       [0.86821353],\n",
       "       [0.06846705],\n",
       "       [0.0193904 ],\n",
       "       [0.71096176],\n",
       "       [0.11030775],\n",
       "       [0.24106151],\n",
       "       [0.96476865],\n",
       "       [0.29941806],\n",
       "       [0.99974173],\n",
       "       [0.99990225],\n",
       "       [0.23631802],\n",
       "       [0.83276695],\n",
       "       [0.760182  ],\n",
       "       [0.71659315],\n",
       "       [0.7987963 ],\n",
       "       [0.820906  ],\n",
       "       [0.03255758],\n",
       "       [0.2777911 ],\n",
       "       [0.8860761 ],\n",
       "       [0.9994924 ],\n",
       "       [0.9898271 ],\n",
       "       [0.992813  ],\n",
       "       [0.18964079],\n",
       "       [0.25708675],\n",
       "       [0.97227   ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = [1 if x>=0.5 else 0 for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5343137254901961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
